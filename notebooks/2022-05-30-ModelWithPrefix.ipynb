{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# only needed for colab\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import os\n",
    "# os.chdir('/content/drive/MyDrive/TaylorExpansionML')\n",
    "os.chdir(\"..\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import sympy as sp\n",
    "from sympy import sympify, srepr\n",
    "import os\n",
    "from icecream import ic\n",
    "import random\n",
    "\n",
    "from source.data_preparation import sympy_to_prefix, prefix_to_sympy, vectorize_ds, vectorize_sentence, pad_right\n",
    "import functools\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from tqdm.notebook import trange, tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-01 12:06:34.186988: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-06-01 12:06:34.189602: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-01 12:06:34.189612: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from icecream import ic"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "load_data_from_scratch = False\n",
    "export_data = False\n",
    "load_full_data = True\n",
    "check_prefix_recovery = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "if load_data_from_scratch:\n",
    "    data_file = \"data.nosync/data.txt\"\n",
    "    taylor_file = \"data.nosync/data_taylor.txt\"\n",
    "    coeffs_file = \"data.nosync/data_coeffs.txt\"\n",
    "\n",
    "    start = [\"[start]\"]\n",
    "    end = [\"[end]\"]\n",
    "\n",
    "    with open(data_file) as f:\n",
    "        X = f.read().split(\"\\n\")\n",
    "        X = np.array(X)\n",
    "        X = X[:-1]   # somehow last entry is empty\n",
    "        X = [sympify(xi) for xi in X]\n",
    "\n",
    "    with open(taylor_file) as f:\n",
    "        y_taylor = f.read().split(\"\\n\")\n",
    "        y_taylor = np.array(y_taylor)\n",
    "        y_taylor = y_taylor[:-1]\n",
    "        y_taylor = [sympify(yi) for yi in y_taylor]\n",
    "\n",
    "    with open(coeffs_file) as f:\n",
    "        y_coeffs = f.read().split(\"\\n\")\n",
    "        y_coeffs = y_coeffs[:-1]\n",
    "        for i, y in enumerate(y_coeffs):\n",
    "            y = sympify(y)\n",
    "            # add start end tokens and remove whitespaces\n",
    "            y_coeffs[i] = y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "if load_data_from_scratch:\n",
    "    train_val_test = [0.8, 0.1, 0.1]\n",
    "    assert np.sum(train_val_test) == 1\n",
    "    assert len(X) == len(y_taylor)\n",
    "    assert len(X) == len(y_coeffs)\n",
    "    total_len = len(X)\n",
    "    train_idx, val_test_idx = train_test_split(\n",
    "        np.array(range(total_len)),\n",
    "        test_size=train_val_test[1]+train_val_test[2],\n",
    "        random_state=42\n",
    "    )\n",
    "    val_idx, test_idx = train_test_split(\n",
    "        val_test_idx,\n",
    "        test_size=(train_val_test[2] / (train_val_test[1]+train_val_test[2]))\n",
    "    )\n",
    "    X_train = [X[i] for i in train_idx]\n",
    "    y_taylor_train = [y_taylor[i] for i in train_idx]\n",
    "    y_coeffs_train = [y_coeffs[i] for i in train_idx]\n",
    "\n",
    "    X_val = [X[i] for i in val_idx]\n",
    "    y_taylor_val = [y_taylor[i] for i in val_idx]\n",
    "    y_coeffs_val = [y_coeffs[i] for i in val_idx]\n",
    "\n",
    "    X_test = [X[i] for i in test_idx]\n",
    "    y_taylor_test = [y_taylor[i] for i in test_idx]\n",
    "    y_coeffs_test = [y_coeffs[i] for i in test_idx]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "X_pickle_file_train = \"data.nosync/data_train.pickle\"\n",
    "y_taylor_pickle_file_train = \"data.nosync/data_taylor_train.pickle\"\n",
    "y_coeffs_pickle_file_train = \"data.nosync/data_coeffs_train.pickle\"\n",
    "\n",
    "X_pickle_file_small_train = \"data.nosync/data_train_small.pickle\"\n",
    "y_taylor_pickle_file_small_train = \"data.nosync/data_taylor_train_small.pickle\"\n",
    "y_coeffs_pickle_file_small_train = \"data.nosync/data_coeffs_train_small.pickle\"\n",
    "\n",
    "X_pickle_file_val = \"data.nosync/data_val.pickle\"\n",
    "y_taylor_pickle_file_val = \"data.nosync/data_taylor_val.pickle\"\n",
    "y_coeffs_pickle_file_val = \"data.nosync/data_coeffs_val.pickle\"\n",
    "\n",
    "X_pickle_file_test = \"data.nosync/data_test.pickle\"\n",
    "y_taylor_pickle_file_test = \"data.nosync/data_taylor_test.pickle\"\n",
    "y_coeffs_pickle_file_test = \"data.nosync/data_coeffs_test.pickle\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "if export_data:\n",
    "    with open(X_pickle_file_train, \"wb\") as f:\n",
    "        pickle.dump(X_train, f)\n",
    "    with open(y_taylor_pickle_file_train, \"wb\") as f:\n",
    "        pickle.dump(y_taylor_train, f)\n",
    "    with open(y_coeffs_pickle_file_train, \"wb\") as f:\n",
    "        pickle.dump(y_coeffs_train, f)\n",
    "\n",
    "    with open(X_pickle_file_small_train, \"wb\") as f:\n",
    "        pickle.dump(X_train[0:100], f)\n",
    "    with open(y_taylor_pickle_file_small_train, \"wb\") as f:\n",
    "        pickle.dump(y_taylor_train[0:100], f)\n",
    "    with open(y_coeffs_pickle_file_small_train, \"wb\") as f:\n",
    "        pickle.dump(y_coeffs_train[0:100], f)\n",
    "\n",
    "    with open(X_pickle_file_val, \"wb\") as f:\n",
    "        pickle.dump(X_val, f)\n",
    "    with open(y_taylor_pickle_file_val, \"wb\") as f:\n",
    "        pickle.dump(y_taylor_val, f)\n",
    "    with open(y_coeffs_pickle_file_val, \"wb\") as f:\n",
    "        pickle.dump(y_coeffs_val, f)\n",
    "\n",
    "    with open(X_pickle_file_test, \"wb\") as f:\n",
    "        pickle.dump(X_test, f)\n",
    "    with open(y_taylor_pickle_file_test, \"wb\") as f:\n",
    "        pickle.dump(y_taylor_test, f)\n",
    "    with open(y_coeffs_pickle_file_test, \"wb\") as f:\n",
    "        pickle.dump(y_coeffs_test, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "if (not load_data_from_scratch) & (not load_full_data):\n",
    "    with open(X_pickle_file_small_train, \"rb\") as f:\n",
    "        X_train = pickle.load(f)\n",
    "    with open(y_taylor_pickle_file_small_train, \"rb\") as f:\n",
    "        y_taylor_train = pickle.load(f)\n",
    "    with open(y_coeffs_pickle_file_small_train, \"rb\") as f:\n",
    "        y_coeffs_train = pickle.load(f)\n",
    "\n",
    "    with open(X_pickle_file_test, \"rb\") as f:\n",
    "        X_test = pickle.load(f)\n",
    "    with open(y_taylor_pickle_file_test, \"rb\") as f:\n",
    "        y_taylor_test = pickle.load(f)\n",
    "    with open(y_coeffs_pickle_file_test, \"rb\") as f:\n",
    "        y_coeffs_test = pickle.load(f)\n",
    "\n",
    "elif load_full_data:\n",
    "    with open(X_pickle_file_train, \"rb\") as f:\n",
    "        X_train = pickle.load(f)\n",
    "    with open(y_taylor_pickle_file_train, \"rb\") as f:\n",
    "        y_taylor_train = pickle.load(f)\n",
    "    with open(y_coeffs_pickle_file_train, \"rb\") as f:\n",
    "        y_coeffs_train = pickle.load(f)\n",
    "    with open(X_pickle_file_val, \"rb\") as f:\n",
    "        X_val = pickle.load(f)\n",
    "    with open(y_taylor_pickle_file_val, \"rb\") as f:\n",
    "        y_taylor_val = pickle.load(f)\n",
    "    with open(y_coeffs_pickle_file_val, \"rb\") as f:\n",
    "        y_coeffs_val = pickle.load(f)\n",
    "    with open(X_pickle_file_test, \"rb\") as f:\n",
    "        X_test = pickle.load(f)\n",
    "    with open(y_taylor_pickle_file_test, \"rb\") as f:\n",
    "        y_taylor_test = pickle.load(f)\n",
    "    with open(y_coeffs_pickle_file_test, \"rb\") as f:\n",
    "        y_coeffs_test = pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| len(X_train): 24000\n",
      "ic| len(X_val): 3000\n",
      "ic| len(X_test): 3000\n",
      "ic| len(y_taylor_train): 24000\n",
      "ic| len(y_taylor_val): 3000\n",
      "ic| len(y_taylor_test): 3000\n"
     ]
    },
    {
     "data": {
      "text/plain": "3000"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ic(len(X_train))\n",
    "ic(len(X_val))\n",
    "\n",
    "ic(len(X_test))\n",
    "\n",
    "ic(len(y_taylor_train))\n",
    "ic(len(y_taylor_val))\n",
    "ic(len(y_taylor_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "X_prefix_train = [sympy_to_prefix(x) for x in X_train]\n",
    "y_taylor_prefix_train = [sympy_to_prefix(y) for y in y_taylor_train]\n",
    "\n",
    "X_prefix_val = [sympy_to_prefix(x) for x in X_val]\n",
    "y_taylor_prefix_val = [sympy_to_prefix(y) for y in y_taylor_val]\n",
    "\n",
    "X_prefix_test = [sympy_to_prefix(x) for x in X_test]\n",
    "y_taylor_prefix_test = [sympy_to_prefix(y) for y in y_taylor_test]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "if check_prefix_recovery:\n",
    "    X_prefix_train_recovered = [prefix_to_sympy(x) for x in X_prefix_train]\n",
    "    y_taylor_prefix_train_recovered = [prefix_to_sympy(y) for y in y_taylor_prefix_train]\n",
    "\n",
    "    X_prefix_test_recovered = [prefix_to_sympy(x) for x in X_prefix_test]\n",
    "    y_taylor_prefix_test_recovered = [prefix_to_sympy(y) for y in y_taylor_prefix_test]\n",
    "\n",
    "\n",
    "    assert X_prefix_train_recovered == X_train\n",
    "    assert y_taylor_prefix_train_recovered == y_taylor_train\n",
    "    print(\"All train data correctly recovered with `prefix_to_sympy`\")\n",
    "\n",
    "    assert X_prefix_test_recovered == X_test\n",
    "    assert y_taylor_prefix_test_recovered == y_taylor_test\n",
    "\n",
    "    print(\"All test data correctly recovered with `prefix_to_sympy`\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "def clean_data(X, y, max_len=200, del_manual_idxs=[]):\n",
    "    \"\"\" Clean up data\n",
    "    - manually remove indices from `del_manual_idxs`\n",
    "    - remove where prefix notation is longer than 200\n",
    "    \"\"\"\n",
    "    previous_length = len(X)\n",
    "\n",
    "    X = [val for i, val in enumerate(X) if i not in del_manual_idxs]\n",
    "    y = [val for i, val in enumerate(y) if i not in del_manual_idxs]\n",
    "\n",
    "    X_too_long_idxs = too_long(X, max_len)\n",
    "    X = [val for i, val in enumerate(X) if i not in X_too_long_idxs]\n",
    "    y = [val for i, val in enumerate(y) if i not in X_too_long_idxs]\n",
    "\n",
    "    y_too_long_idxs = too_long(y, max_len)\n",
    "    X = [val for i, val in enumerate(X) if i not in y_too_long_idxs]\n",
    "    y = [val for i, val in enumerate(y) if i not in y_too_long_idxs]\n",
    "\n",
    "    total_removed = len(del_manual_idxs) + len(X_too_long_idxs) + len(y_too_long_idxs)\n",
    "    del_percentage = total_removed / previous_length\n",
    "    print(\"Deleted \", total_removed, \"elements\", \"of\", previous_length)\n",
    "    print(\"That's \", round(del_percentage*100, 1), \"%\")\n",
    "\n",
    "    return (X, y)\n",
    "\n",
    "\n",
    "def too_long(data, max_len):\n",
    "    \"\"\"return indices where len(data)>max_len\"\"\"\n",
    "    return [i for (i, x) in enumerate(data) if len(x)>max_len]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:\n",
      "Deleted  0 elements of 21621\n",
      "That's  0.0 %\n",
      "val:\n",
      "Deleted  322 elements of 3000\n",
      "That's  10.7 %\n",
      "test:\n",
      "Deleted  0 elements of 2705\n",
      "That's  0.0 %\n"
     ]
    }
   ],
   "source": [
    "del_manual_idxs = [\n",
    "]\n",
    "max_len = 200\n",
    "print(\"train:\")\n",
    "X_prefix_train, y_taylor_prefix_train = clean_data(X_prefix_train, y_taylor_prefix_train, del_manual_idxs=del_manual_idxs, max_len=max_len)\n",
    "\n",
    "print(\"val:\")\n",
    "X_prefix_val, y_taylor_prefix_val = clean_data(X_prefix_val, y_taylor_prefix_val, del_manual_idxs=del_manual_idxs, max_len=max_len)\n",
    "\n",
    "print(\"test:\")\n",
    "X_prefix_test, y_taylor_prefix_test = clean_data(X_prefix_test, y_taylor_prefix_test, max_len=max_len)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "data": {
      "text/plain": "21621"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_prefix_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Vectorization"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "def make_one_string(word_arr, start_end=False):\n",
    "    ret = functools.reduce(lambda a, b: a+ \" \" + b, word_arr)\n",
    "    if start_end:\n",
    "        ret = \"[start]\" + \" \" + ret + \" \" + \"[end]\"\n",
    "    return ret"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "# vocab_size = 1000\n",
    "sequence_length = 200\n",
    "\n",
    "numbers_tokens = ['int ', 's+', 's-'] +[str(i) for i in range(0,10)]\n",
    "numbers_tokens = make_one_string(numbers_tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-01 12:11:16.688967: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-01 12:11:16.689144: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-01 12:11:16.689189: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-06-01 12:11:16.689225: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-06-01 12:11:16.689261: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-06-01 12:11:16.689297: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-06-01 12:11:16.689329: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-06-01 12:11:16.689362: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-06-01 12:11:16.689394: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-06-01 12:11:16.689402: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-06-01 12:11:16.689660: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "X_vectorize_layer = TextVectorization(\n",
    "    # max_tokens=vocab_size,\n",
    "    standardize=None,\n",
    "    split='whitespace',\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)\n",
    "\n",
    "y_vectorize_layer = TextVectorization(\n",
    "    # max_tokens=vocab_size,\n",
    "    standardize=None,\n",
    "    split='whitespace',\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length+1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "X_prefix_train_str = [make_one_string(x) for x in X_prefix_train]\n",
    "y_prefix_train_str = [make_one_string(y, start_end=True) for y in y_taylor_prefix_train]\n",
    "\n",
    "X_prefix_val_str = [make_one_string(x) for x in X_prefix_val]\n",
    "y_prefix_val_str = [make_one_string(y, start_end=True) for y in y_taylor_prefix_val]\n",
    "\n",
    "X_prefix_test_str = [make_one_string(x) for x in X_prefix_test]\n",
    "y_prefix_test_str = [make_one_string(y, start_end=True) for y in y_taylor_prefix_test]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "data": {
      "text/plain": "'mul x pow add d add sinh x tan x int s- 1'"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_prefix_train_str[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "data": {
      "text/plain": "'[start] add mul x pow d int s- 1 add mul int s- 2 mul pow d int s- 2 pow x int s+ 2 add mul int s+ 4 mul pow d int s- 3 pow x int s+ 3 mul mul int s- 1 pow int s+ 2 4 int s- 1 mul pow d int s- 2 mul pow x int s+ 4 add int s+ 1 2 mul int s+ 1 9 2 pow d int s- 2 [end]'"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_prefix_train_str[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "numbers = [str(i) for i in range(0,10)]\n",
    "numbers = 'int s+ s_ ' + make_one_string(numbers)\n",
    "X_vectorize_layer.adapt([numbers]+X_prefix_train_str)\n",
    "y_vectorize_layer.adapt([numbers]+y_prefix_train_str)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(1, 200), dtype=int64, numpy=\narray([[ 3,  2,  4,  8, 10,  8, 19,  2, 20,  2,  5,  9,  6,  0,  0,  0,\n         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n         0,  0,  0,  0,  0,  0,  0,  0]])>"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_enc0 = X_vectorize_layer(X_prefix_train_str[0:1])\n",
    "X_enc0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(1, 201), dtype=int64, numpy=\narray([[11,  8,  3, 10,  5, 16,  2,  7,  6,  8,  3,  2,  7,  9,  3,  5,\n        16,  2,  7,  9,  5, 10,  2,  4,  9,  8,  3,  2,  4, 13,  3,  5,\n        16,  2,  7, 14,  5, 10,  2,  4, 14,  3,  3,  2,  7,  6,  5,  2,\n         4,  9, 13,  2,  7,  6,  3,  5, 16,  2,  7,  9,  3,  5, 10,  2,\n         4, 13,  8,  2,  4,  6,  9,  3,  2,  4,  6, 32,  9,  5, 16,  2,\n         7,  9, 12,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n         0,  0,  0,  0,  0,  0,  0,  0,  0]])>"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_enc0 = y_vectorize_layer(y_prefix_train_str[0:1])\n",
    "y_enc0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "X_vocabulary = np.array(X_vectorize_layer.get_vocabulary())\n",
    "y_vocabulary = np.array(y_vectorize_layer.get_vocabulary())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "def unvectorize(sentence, vocab):\n",
    "    return list(vocab[tuple(sentence)])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "X_train_vec = X_vectorize_layer(X_prefix_train_str)\n",
    "y_train_vec = y_vectorize_layer(y_prefix_train_str)\n",
    "\n",
    "X_val_vec = X_vectorize_layer(X_prefix_val_str)\n",
    "y_val_vec = y_vectorize_layer(y_prefix_val_str)\n",
    "\n",
    "X_test_vec = X_vectorize_layer(X_prefix_test_str)\n",
    "y_test_vec = y_vectorize_layer(y_prefix_test_str)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "batch_size=128\n",
    "\n",
    "def format_dataset(X, y):\n",
    "    return ({\"encoder_inputs\": X, \"decoder_inputs\": y[:, :-1],}, y[:, 1:])\n",
    "\n",
    "def make_dataset(X, y):\n",
    "    dataset = format_dataset(X,y)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(dataset)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()\n",
    "\n",
    "train_ds = make_dataset(X_train_vec, y_train_vec)\n",
    "val_ds = make_dataset(X_val_vec, y_val_vec)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs[\"encoder_inputs\"].shape: (128, 200)\n",
      "inputs[\"decoder_inputs\"].shape: (128, 200)\n",
      "targets.shape: (128, 200)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-01 12:11:59.620685: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds.take(1):\n",
    "    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n",
    "    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n",
    "    print(f\"targets.shape: {targets.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Transformer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super(TransformerEncoder, self).__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = tf.keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=\"int32\")\n",
    "        attention_output = self.attention(\n",
    "            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n",
    "        )\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "\n",
    "\n",
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
    "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=vocab_size, output_dim=embed_dim\n",
    "        )\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=embed_dim\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "\n",
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
    "        super(TransformerDecoder, self).__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.attention_2 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = tf.keras.Sequential(\n",
    "            [layers.Dense(latent_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.layernorm_3 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
    "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
    "\n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n",
    "        )\n",
    "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "\n",
    "        attention_output_2 = self.attention_2(\n",
    "            query=out_1,\n",
    "            value=encoder_outputs,\n",
    "            key=encoder_outputs,\n",
    "            attention_mask=padding_mask,\n",
    "        )\n",
    "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
    "\n",
    "        proj_output = self.dense_proj(out_2)\n",
    "        return self.layernorm_3(out_2 + proj_output)\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
    "            axis=0,\n",
    "        )\n",
    "        return tf.tile(mask, mult)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras_tuner in /home/boggog/anaconda3/lib/python3.9/site-packages (1.1.2)\r\n",
      "Requirement already satisfied: requests in /home/boggog/anaconda3/lib/python3.9/site-packages (from keras_tuner) (2.27.1)\r\n",
      "Requirement already satisfied: numpy in /home/boggog/anaconda3/lib/python3.9/site-packages (from keras_tuner) (1.21.5)\r\n",
      "Requirement already satisfied: ipython in /home/boggog/anaconda3/lib/python3.9/site-packages (from keras_tuner) (8.2.0)\r\n",
      "Requirement already satisfied: packaging in /home/boggog/anaconda3/lib/python3.9/site-packages (from keras_tuner) (21.3)\r\n",
      "Requirement already satisfied: tensorboard in /home/boggog/anaconda3/lib/python3.9/site-packages (from keras_tuner) (2.9.0)\r\n",
      "Requirement already satisfied: kt-legacy in /home/boggog/anaconda3/lib/python3.9/site-packages (from keras_tuner) (1.0.4)\r\n",
      "Requirement already satisfied: stack-data in /home/boggog/anaconda3/lib/python3.9/site-packages (from ipython->keras_tuner) (0.2.0)\r\n",
      "Requirement already satisfied: pickleshare in /home/boggog/anaconda3/lib/python3.9/site-packages (from ipython->keras_tuner) (0.7.5)\r\n",
      "Requirement already satisfied: decorator in /home/boggog/anaconda3/lib/python3.9/site-packages (from ipython->keras_tuner) (5.1.1)\r\n",
      "Requirement already satisfied: matplotlib-inline in /home/boggog/anaconda3/lib/python3.9/site-packages (from ipython->keras_tuner) (0.1.2)\r\n",
      "Requirement already satisfied: backcall in /home/boggog/anaconda3/lib/python3.9/site-packages (from ipython->keras_tuner) (0.2.0)\r\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/boggog/anaconda3/lib/python3.9/site-packages (from ipython->keras_tuner) (2.11.2)\r\n",
      "Requirement already satisfied: setuptools>=18.5 in /home/boggog/anaconda3/lib/python3.9/site-packages (from ipython->keras_tuner) (61.2.0)\r\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /home/boggog/anaconda3/lib/python3.9/site-packages (from ipython->keras_tuner) (3.0.20)\r\n",
      "Requirement already satisfied: jedi>=0.16 in /home/boggog/anaconda3/lib/python3.9/site-packages (from ipython->keras_tuner) (0.18.1)\r\n",
      "Requirement already satisfied: pexpect>4.3 in /home/boggog/anaconda3/lib/python3.9/site-packages (from ipython->keras_tuner) (4.8.0)\r\n",
      "Requirement already satisfied: traitlets>=5 in /home/boggog/anaconda3/lib/python3.9/site-packages (from ipython->keras_tuner) (5.1.1)\r\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /home/boggog/anaconda3/lib/python3.9/site-packages (from jedi>=0.16->ipython->keras_tuner) (0.8.3)\r\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/boggog/anaconda3/lib/python3.9/site-packages (from pexpect>4.3->ipython->keras_tuner) (0.7.0)\r\n",
      "Requirement already satisfied: wcwidth in /home/boggog/anaconda3/lib/python3.9/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->keras_tuner) (0.2.5)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/boggog/anaconda3/lib/python3.9/site-packages (from packaging->keras_tuner) (3.0.4)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/boggog/anaconda3/lib/python3.9/site-packages (from requests->keras_tuner) (1.26.9)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/boggog/anaconda3/lib/python3.9/site-packages (from requests->keras_tuner) (3.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/boggog/anaconda3/lib/python3.9/site-packages (from requests->keras_tuner) (2021.10.8)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/boggog/anaconda3/lib/python3.9/site-packages (from requests->keras_tuner) (2.0.4)\r\n",
      "Requirement already satisfied: executing in /home/boggog/anaconda3/lib/python3.9/site-packages (from stack-data->ipython->keras_tuner) (0.8.3)\r\n",
      "Requirement already satisfied: asttokens in /home/boggog/anaconda3/lib/python3.9/site-packages (from stack-data->ipython->keras_tuner) (2.0.5)\r\n",
      "Requirement already satisfied: pure-eval in /home/boggog/anaconda3/lib/python3.9/site-packages (from stack-data->ipython->keras_tuner) (0.2.2)\r\n",
      "Requirement already satisfied: six in /home/boggog/anaconda3/lib/python3.9/site-packages (from asttokens->stack-data->ipython->keras_tuner) (1.16.0)\r\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/boggog/anaconda3/lib/python3.9/site-packages (from tensorboard->keras_tuner) (0.4.6)\r\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /home/boggog/anaconda3/lib/python3.9/site-packages (from tensorboard->keras_tuner) (3.19.1)\r\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/boggog/anaconda3/lib/python3.9/site-packages (from tensorboard->keras_tuner) (1.8.1)\r\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /home/boggog/anaconda3/lib/python3.9/site-packages (from tensorboard->keras_tuner) (1.42.0)\r\n",
      "Requirement already satisfied: wheel>=0.26 in /home/boggog/anaconda3/lib/python3.9/site-packages (from tensorboard->keras_tuner) (0.37.1)\r\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/boggog/anaconda3/lib/python3.9/site-packages (from tensorboard->keras_tuner) (2.0.3)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/boggog/anaconda3/lib/python3.9/site-packages (from tensorboard->keras_tuner) (0.6.1)\r\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/boggog/anaconda3/lib/python3.9/site-packages (from tensorboard->keras_tuner) (1.0.0)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/boggog/anaconda3/lib/python3.9/site-packages (from tensorboard->keras_tuner) (3.3.4)\r\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/boggog/anaconda3/lib/python3.9/site-packages (from tensorboard->keras_tuner) (1.33.0)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/boggog/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard->keras_tuner) (0.2.8)\r\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/boggog/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard->keras_tuner) (4.2.2)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/boggog/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard->keras_tuner) (4.7.2)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/boggog/anaconda3/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras_tuner) (1.3.1)\r\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/boggog/anaconda3/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->keras_tuner) (0.4.8)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/boggog/anaconda3/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras_tuner) (3.2.0)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras_tuner"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "import keras_tuner as kt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "    # embed_dim = 256\n",
    "    # latent_dim = 512*2\n",
    "    # num_heads = 7\n",
    "    vocab_size = 200\n",
    "\n",
    "    embed_dim = hp.Int('embed_dim', min_value=256, max_value=512, step=56)\n",
    "    num_heads = hp.Int('num_heads', min_value=4, max_value=10, step=1)\n",
    "    latent_dim = hp.Int('latent_dim', min_value=256, max_value=512*4, step=256)\n",
    "\n",
    "    \n",
    "    encoder_inputs = tf.keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
    "    x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
    "    encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n",
    "    encoder = tf.keras.Model(encoder_inputs, encoder_outputs)\n",
    "    \n",
    "    decoder_inputs = tf.keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
    "    encoded_seq_inputs = tf.keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n",
    "    x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
    "    x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
    "    decoder = tf.keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
    "    \n",
    "    decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
    "    transformer = tf.keras.Model(\n",
    "        [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",
    "    )\n",
    "    transformer.compile(\n",
    "        \"rmsprop\",\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return transformer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(model_builder,\n",
    "                    overwrite=True,\n",
    "                    objective=\"val_loss\",\n",
    "                    max_epochs=200,\n",
    "                    factor=3,\n",
    "                    directory=\"models/2022-05-31-TaylorModelPrefix\",\n",
    "                    project_name=\"2022-05-31-TaylorModelPrefix\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search: Running Trial #1\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "312               |?                 |embed_dim\n",
      "10                |?                 |num_heads\n",
      "2048              |?                 |latent_dim\n",
      "3                 |?                 |tuner/epochs\n",
      "0                 |?                 |tuner/initial_epoch\n",
      "4                 |?                 |tuner/bracket\n",
      "0                 |?                 |tuner/round\n",
      "\n",
      "Epoch 1/3\n",
      " 8/62 [==>...........................] - ETA: 6:39 - loss: 1.7538 - accuracy: 0.1172"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [79]\u001B[0m, in \u001B[0;36m<cell line: 4>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m stop_early \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mcallbacks\u001B[38;5;241m.\u001B[39mEarlyStopping(monitor\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mval_loss\u001B[39m\u001B[38;5;124m\"\u001B[39m, patience\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m50\u001B[39m)\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=20, min_lr=1e-5) \u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m \u001B[43mtuner\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msearch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_ds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m             \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_ds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m             \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m200\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m             \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[43m                 \u001B[49m\u001B[43mstop_early\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;66;43;03m# tf.keras.callbacks.TensorBoard(\"logs/2022-05-31-TaylorModelPrefix\")\u001B[39;49;00m\n\u001B[1;32m     10\u001B[0m \u001B[43m             \u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     11\u001B[0m \u001B[43m             \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.9/site-packages/keras_tuner/engine/base_tuner.py:179\u001B[0m, in \u001B[0;36mBaseTuner.search\u001B[0;34m(self, *fit_args, **fit_kwargs)\u001B[0m\n\u001B[1;32m    176\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[1;32m    178\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_trial_begin(trial)\n\u001B[0;32m--> 179\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_trial\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfit_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfit_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    180\u001B[0m \u001B[38;5;66;03m# `results` is None indicates user updated oracle in `run_trial()`.\u001B[39;00m\n\u001B[1;32m    181\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m results \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.9/site-packages/keras_tuner/tuners/hyperband.py:384\u001B[0m, in \u001B[0;36mHyperband.run_trial\u001B[0;34m(self, trial, *fit_args, **fit_kwargs)\u001B[0m\n\u001B[1;32m    382\u001B[0m     fit_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mepochs\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m hp\u001B[38;5;241m.\u001B[39mvalues[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtuner/epochs\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m    383\u001B[0m     fit_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minitial_epoch\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m hp\u001B[38;5;241m.\u001B[39mvalues[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtuner/initial_epoch\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m--> 384\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mHyperband\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_trial\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfit_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfit_kwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.9/site-packages/keras_tuner/engine/tuner.py:294\u001B[0m, in \u001B[0;36mTuner.run_trial\u001B[0;34m(self, trial, *args, **kwargs)\u001B[0m\n\u001B[1;32m    292\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mappend(model_checkpoint)\n\u001B[1;32m    293\u001B[0m     copied_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcallbacks\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m callbacks\n\u001B[0;32m--> 294\u001B[0m     obj_value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_build_and_fit_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcopied_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    296\u001B[0m     histories\u001B[38;5;241m.\u001B[39mappend(obj_value)\n\u001B[1;32m    297\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m histories\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.9/site-packages/keras_tuner/engine/tuner.py:222\u001B[0m, in \u001B[0;36mTuner._build_and_fit_model\u001B[0;34m(self, trial, *args, **kwargs)\u001B[0m\n\u001B[1;32m    220\u001B[0m hp \u001B[38;5;241m=\u001B[39m trial\u001B[38;5;241m.\u001B[39mhyperparameters\n\u001B[1;32m    221\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_try_build(hp)\n\u001B[0;32m--> 222\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhypermodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    223\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m tuner_utils\u001B[38;5;241m.\u001B[39mconvert_to_metrics_dict(\n\u001B[1;32m    224\u001B[0m     results, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moracle\u001B[38;5;241m.\u001B[39mobjective, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mHyperModel.fit()\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    225\u001B[0m )\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.9/site-packages/keras_tuner/engine/hypermodel.py:137\u001B[0m, in \u001B[0;36mHyperModel.fit\u001B[0;34m(self, hp, model, *args, **kwargs)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfit\u001B[39m(\u001B[38;5;28mself\u001B[39m, hp, model, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;124;03m\"\"\"Train the model.\u001B[39;00m\n\u001B[1;32m    115\u001B[0m \n\u001B[1;32m    116\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    135\u001B[0m \u001B[38;5;124;03m        If return a float, it should be the `objective` value.\u001B[39;00m\n\u001B[1;32m    136\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 137\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py:64\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     62\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 64\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     65\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint: disable=broad-except\u001B[39;00m\n\u001B[1;32m     66\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.9/site-packages/keras/engine/training.py:1409\u001B[0m, in \u001B[0;36mModel.fit\u001B[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[1;32m   1402\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mexperimental\u001B[38;5;241m.\u001B[39mTrace(\n\u001B[1;32m   1403\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m   1404\u001B[0m     epoch_num\u001B[38;5;241m=\u001B[39mepoch,\n\u001B[1;32m   1405\u001B[0m     step_num\u001B[38;5;241m=\u001B[39mstep,\n\u001B[1;32m   1406\u001B[0m     batch_size\u001B[38;5;241m=\u001B[39mbatch_size,\n\u001B[1;32m   1407\u001B[0m     _r\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m):\n\u001B[1;32m   1408\u001B[0m   callbacks\u001B[38;5;241m.\u001B[39mon_train_batch_begin(step)\n\u001B[0;32m-> 1409\u001B[0m   tmp_logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1410\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m data_handler\u001B[38;5;241m.\u001B[39mshould_sync:\n\u001B[1;32m   1411\u001B[0m     context\u001B[38;5;241m.\u001B[39masync_wait()\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    148\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    149\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 150\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:915\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    912\u001B[0m compiler \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxla\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnonXla\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    914\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m OptionalXlaContext(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile):\n\u001B[0;32m--> 915\u001B[0m   result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    917\u001B[0m new_tracing_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexperimental_get_tracing_count()\n\u001B[1;32m    918\u001B[0m without_tracing \u001B[38;5;241m=\u001B[39m (tracing_count \u001B[38;5;241m==\u001B[39m new_tracing_count)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:947\u001B[0m, in \u001B[0;36mFunction._call\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    944\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n\u001B[1;32m    945\u001B[0m   \u001B[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001B[39;00m\n\u001B[1;32m    946\u001B[0m   \u001B[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001B[39;00m\n\u001B[0;32m--> 947\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_stateless_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# pylint: disable=not-callable\u001B[39;00m\n\u001B[1;32m    948\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stateful_fn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    949\u001B[0m   \u001B[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001B[39;00m\n\u001B[1;32m    950\u001B[0m   \u001B[38;5;66;03m# in parallel.\u001B[39;00m\n\u001B[1;32m    951\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/function.py:2453\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   2450\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[1;32m   2451\u001B[0m   (graph_function,\n\u001B[1;32m   2452\u001B[0m    filtered_flat_args) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_maybe_define_function(args, kwargs)\n\u001B[0;32m-> 2453\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mgraph_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_flat\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2454\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfiltered_flat_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcaptured_inputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgraph_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcaptured_inputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/function.py:1860\u001B[0m, in \u001B[0;36mConcreteFunction._call_flat\u001B[0;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[1;32m   1856\u001B[0m possible_gradient_type \u001B[38;5;241m=\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPossibleTapeGradientTypes(args)\n\u001B[1;32m   1857\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (possible_gradient_type \u001B[38;5;241m==\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001B[1;32m   1858\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m executing_eagerly):\n\u001B[1;32m   1859\u001B[0m   \u001B[38;5;66;03m# No tape is watching; skip to running the function.\u001B[39;00m\n\u001B[0;32m-> 1860\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_call_outputs(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_inference_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1861\u001B[0m \u001B[43m      \u001B[49m\u001B[43mctx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcancellation_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcancellation_manager\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m   1862\u001B[0m forward_backward \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_select_forward_and_backward_functions(\n\u001B[1;32m   1863\u001B[0m     args,\n\u001B[1;32m   1864\u001B[0m     possible_gradient_type,\n\u001B[1;32m   1865\u001B[0m     executing_eagerly)\n\u001B[1;32m   1866\u001B[0m forward_function, args_with_tangents \u001B[38;5;241m=\u001B[39m forward_backward\u001B[38;5;241m.\u001B[39mforward()\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/function.py:497\u001B[0m, in \u001B[0;36m_EagerDefinedFunction.call\u001B[0;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[1;32m    495\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _InterpolateFunctionError(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    496\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m cancellation_manager \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 497\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mexecute\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    498\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msignature\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    499\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_num_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    500\u001B[0m \u001B[43m        \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    501\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattrs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    502\u001B[0m \u001B[43m        \u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mctx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    503\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    504\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m execute\u001B[38;5;241m.\u001B[39mexecute_with_cancellation(\n\u001B[1;32m    505\u001B[0m         \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msignature\u001B[38;5;241m.\u001B[39mname),\n\u001B[1;32m    506\u001B[0m         num_outputs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_outputs,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    509\u001B[0m         ctx\u001B[38;5;241m=\u001B[39mctx,\n\u001B[1;32m    510\u001B[0m         cancellation_manager\u001B[38;5;241m=\u001B[39mcancellation_manager)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:54\u001B[0m, in \u001B[0;36mquick_execute\u001B[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m   ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[0;32m---> 54\u001B[0m   tensors \u001B[38;5;241m=\u001B[39m \u001B[43mpywrap_tfe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTFE_Py_Execute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mop_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     55\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     56\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     57\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=50)\n",
    "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=20, min_lr=1e-5) \n",
    "\n",
    "tuner.search(train_ds,\n",
    "             validation_data=val_ds,\n",
    "             epochs=200,\n",
    "             callbacks=[\n",
    "                 stop_early,\n",
    "                # tf.keras.callbacks.TensorBoard(\"logs/2022-05-31-TaylorModelPrefix\")\n",
    "             ],\n",
    "             )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "tmp_hps = kt.HyperParameters()\n",
    "tmp_hps.values = {\n",
    "    'embed_dim': 200,\n",
    "    'num_heads': 5,\n",
    "    'latent_dim': 512,\n",
    "           }"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " positional_embedding_2 (Positi  (None, None, 200)   80000       ['encoder_inputs[0][0]']         \n",
      " onalEmbedding)                                                                                   \n",
      "                                                                                                  \n",
      " decoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " transformer_encoder_1 (Transfo  (None, None, 200)   1009512     ['positional_embedding_2[0][0]'] \n",
      " rmerEncoder)                                                                                     \n",
      "                                                                                                  \n",
      " model_3 (Functional)           (None, None, 200)    1933312     ['decoder_inputs[0][0]',         \n",
      "                                                                  'transformer_encoder_1[0][0]']  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,022,824\n",
      "Trainable params: 3,022,824\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = model_builder(tmp_hps)\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tf.keras.callbacks.ModelCheckpoint(\n",
    "    \"models/2022-05-31-TaylorModelPrefix_retrained_best/\",\n",
    "    monitor=\"val_loss\",\n",
    "    verbose=0,\n",
    "    save_best_only=True,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "62/62 [==============================] - 122s 2s/step - loss: 0.6221 - accuracy: 0.4277 - val_loss: 0.4751 - val_accuracy: 0.5049\n",
      "Epoch 2/2\n",
      "62/62 [==============================] - 114s 2s/step - loss: 0.4131 - accuracy: 0.5193 - val_loss: 0.4325 - val_accuracy: 0.5341\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x7f51d46e1070>"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_ds, epochs=2, validation_data=val_ds)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "model.save_weights(\"models/test_model_weights.h5\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "model_reloaded = model_builder(tmp_hps)\n",
    "model_reloaded.load_weights(\"models/test_model_weights.h5\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Testing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [],
   "source": [
    "max_decoded_sentence_length = 200\n",
    "def apply_transformer(X, y_vectorize_layer):\n",
    "    decoded_sentence = [\"[start]\"]\n",
    "    vocab = y_vectorize_layer.get_vocabulary()\n",
    "    X_tensor = tf.convert_to_tensor([X])\n",
    "\n",
    "    # for i in range(max_decoded_sentence_length):\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence = y_vectorize_layer(decoded_sentence)[:,0:max_decoded_sentence_length]\n",
    "        y_pred = model_reloaded([X_tensor, tokenized_target_sentence])[0,i,:]\n",
    "        y_index = np.argmax(y_pred)\n",
    "        y = vocab[y_index]\n",
    "        decoded_sentence[0] = decoded_sentence[0] + \" \" + y\n",
    "\n",
    "        if y == \"[end]\":\n",
    "            break\n",
    "    return decoded_sentence[0][1:-1] # no start/end\n",
    "    # return  decoded_sentence"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [],
   "source": [
    "def token_accuracy(y_true, y_pred, verbose=0):\n",
    "    \"\"\"\n",
    "    compare two arrays and check how many entries are the same at the same position\n",
    "    \"\"\"\n",
    "    max_ind = np.min([len(y_true), len(y_pred)])\n",
    "    correct_ctr = 0\n",
    "    # ignore [start] and [end]\n",
    "    max_correct = len(y_pred) - 2\n",
    "    for i in range(1, max_ind-1):\n",
    "        if verbose: ic([y_true[i], y_pred[i]])\n",
    "        if y_true[i] == y_pred[i]:\n",
    "            correct_ctr += 1\n",
    "    return correct_ctr / max_correct"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [],
   "source": [
    "y_val_pred = [apply_transformer(x, y_vectorize_layer).split() for x in X_val_vec[0:10]]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| token_acc_list: [0.2,\n",
      "                     0.03314917127071823,\n",
      "                     0.07103825136612021,\n",
      "                     0.06097560975609756,\n",
      "                     0.0582010582010582,\n",
      "                     0.06077348066298342,\n",
      "                     0.11538461538461539,\n",
      "                     0.05025125628140704,\n",
      "                     0.0,\n",
      "                     0.09090909090909091]\n",
      "ic| token_acc: 0.07406825338320909\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.07406825338320909"
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_acc_list = [token_accuracy(y_true, y_pred) for y_true, y_pred in zip(y_taylor_prefix_val[0:10], y_test_pred[0:10])]\n",
    "token_acc = np.mean(token_acc_list)\n",
    "ic(token_acc_list)\n",
    "ic(token_acc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "name": "2022-05-30-ModelWithPrefix.ipynb",
   "provenance": [],
   "collapsed_sections": [
    "OBrxjsuYmTHs"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}