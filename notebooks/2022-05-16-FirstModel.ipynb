{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How to handle unknown words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "import sympy\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sympy import srepr\n",
    "from sympy import preorder_traversal, symbols\n",
    "from sympy.parsing.sympy_parser import parse_expr\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_right(list, total_length=5, const=0):\n",
    "    length = len(list)\n",
    "    values_needed = total_length - length \n",
    "    return np.pad(list, (0, values_needed), mode=\"constant\", constant_values=const) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"data.nosync/data.txt\"\n",
    "taylor_file = \"data.nosync/data_taylor.txt\"\n",
    "coeffs_file = \"data.nosync/data_coeffs.txt\"\n",
    "\n",
    "start = [\"[start]\"]\n",
    "end = [\"[end]\"]\n",
    "\n",
    "with open(data_file) as f:\n",
    "    X = f.read().split(\"\\n\")\n",
    "    X = np.array(X)\n",
    "    X = X[:-1]   # somehow last entry is empty\n",
    "    X = [parse_expr(xi) for xi in X]\n",
    "\n",
    "with open(taylor_file) as f:\n",
    "    y_taylor = f.read().split(\"\\n\")\n",
    "    y_taylor = np.array(y_taylor)\n",
    "    y_taylor = y_taylor[:-1]\n",
    "    y_taylor = [parse_expr(yi) for yi in y_taylor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(coeffs_file) as f:\n",
    "    y_coeffs = f.read().split(\"\\n\")\n",
    "    y_coeffs = y_coeffs[:-1]\n",
    "    for i, y in enumerate(y_coeffs):\n",
    "        y = parse_expr(y)\n",
    "        # add start end tokens and remove whitespaces\n",
    "        y_coeffs[i] = y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "tmp = [[X[i], y_coeffs[i], y_taylor[i]] for i in range(0, len(X))]\n",
    "random.shuffle(tmp)\n",
    "\n",
    "num_train_samples = int(0.90 * len(tmp))\n",
    "\n",
    "train = tmp[0:num_train_samples]\n",
    "test = tmp[num_train_samples:]\n",
    "X_train = [x[0] for x in train]\n",
    "X_test = [x[0] for x in test]\n",
    "y_taylor_train = [x[2] for x in train]\n",
    "y_taylor_test = [x[2] for x in test]\n",
    "print(len(tmp) == len(train) + len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sympy_tokenize(expr, tokens_list=[], depth=0, parent_ind=None):\n",
    "    if (expr.func == sympy.core.symbol.Symbol) | (expr.func == sympy.core.numbers.Integer):\n",
    "        to_append = expr\n",
    "    else:\n",
    "        to_append = expr.func\n",
    "    tokens_list.append(to_append)\n",
    "    for ind, arg in enumerate(expr.args):\n",
    "        sympy_tokenize(arg, tokens_list, depth+1, parent_ind=ind)\n",
    "    return tokens_list\n",
    "\n",
    "def sympy_tokenize_str(sentence):\n",
    "    Xi_tokenized = sympy_tokenize(sentence, tokens_list=[])\n",
    "    Xi_tokenized_str = [str(el) for el in Xi_tokenized]\n",
    "    return Xi_tokenized_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tokenized_str_train = [sympy_tokenize_str(Xi) for Xi in X_train]\n",
    "y_taylor_tokenized_str_train = [start+sympy_tokenize_str(yi)+end for yi in y_taylor_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_X = Word2Vec(sentences=X_tokenized_str_train, vector_size=100, window=5, min_count=1, workers=4)\n",
    "word2vec_y = Word2Vec(sentences=y_taylor_tokenized_str_train, vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sentence(Xi, model):\n",
    "    # 0 reserved for [end], so add 1 to index\n",
    "    Xi_vectorized = [model.wv.key_to_index[word]+1 for word in Xi]\n",
    "    return Xi_vectorized\n",
    "\n",
    "def vectorize(X_tokenized_str, model, sequence_length=25):\n",
    "    X_vectorized = [ vectorize_sentence(sentence, model) for sentence in X_tokenized_str]\n",
    "    # sequence_length = np.max([len(Xi) for Xi in X_vectorized]) + 5\n",
    "    X_vectorized = [pad_right(Xi, sequence_length, const=0) for Xi in X_vectorized]\n",
    "    return X_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 2,  6, 14,  5, 10,  1, 18,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
       " array([ 8,  6, 19,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])]"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_vectorized_train = vectorize(X_tokenized_str_train, word2vec_X, sequence_length=30)\n",
    "y_taylor_vectorized_train = vectorize(y_taylor_tokenized_str_train, word2vec_y, sequence_length=1200)\n",
    "X_vectorized_train[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unvectorize_sentence(Xi, model):\n",
    "    end_ind = np.min(np.where(np.array(Xi) == 0)[0])\n",
    "    Xi_trunc = Xi[0:end_ind]\n",
    "    return [ model.wv.index_to_key[word-1] for word in Xi_trunc]\n",
    "\n",
    "def unvectorize(X_vectorized, model):\n",
    "    X_unvectorized = [unvectorize_sentence(sentence, model) for sentence in X_vectorized]\n",
    "    return X_unvectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "X_unvectorized_train = unvectorize(X_vectorized_train, word2vec_X)\n",
    "print(X_unvectorized_train == X_tokenized_str_train)\n",
    "\n",
    "y_taylor_unvectorized_train = unvectorize(y_taylor_vectorized_train, word2vec_y)\n",
    "print(y_taylor_unvectorized_train == y_taylor_tokenized_str_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=16\n",
    "sequence_length_X = 30\n",
    "sequence_length_y = 1200\n",
    "\n",
    "def format_dataset(X, y):\n",
    "    X = vectorize(X, word2vec_X, sequence_length=sequence_length_X)\n",
    "    y = vectorize(y, word2vec_y, sequence_length=sequence_length_y)\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    return ({\"encoder_inputs\": X, \"decoder_inputs\": y[:, :-1],}, y[:, 1:])\n",
    "\n",
    "def make_dataset(X, y):\n",
    "    dataset = format_dataset(X,y)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(dataset)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()\n",
    "\n",
    "\n",
    "train_ds = make_dataset(X_tokenized_str_train, y_taylor_tokenized_str_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs[\"encoder_inputs\"].shape: (16, 30)\n",
      "inputs[\"decoder_inputs\"].shape: (16, 1199)\n",
      "targets.shape: (16, 1199)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-18 08:36:26.840755: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds.take(1):\n",
    "    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n",
    "    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n",
    "    print(f\"targets.shape: {targets.shape}\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "68a305c3bb66dc100fb8abae87d111d7cb4359f8d5378146abd832e466566bba"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('RandomFunctionGenerator')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
