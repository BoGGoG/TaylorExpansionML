{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How to handle unknown words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "import sympy\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sympy import srepr\n",
    "from sympy import preorder_traversal, symbols\n",
    "from sympy.parsing.sympy_parser import parse_expr\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_right(list, total_length=5, const=0):\n",
    "    length = len(list)\n",
    "    values_needed = total_length - length \n",
    "    if values_needed > 0:\n",
    "        return np.pad(list, (0, values_needed), mode=\"constant\", constant_values=const) \n",
    "    else:\n",
    "        return list[0:total_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"data.nosync/data.txt\"\n",
    "taylor_file = \"data.nosync/data_taylor.txt\"\n",
    "coeffs_file = \"data.nosync/data_coeffs.txt\"\n",
    "\n",
    "start = [\"[start]\"]\n",
    "end = [\"[end]\"]\n",
    "\n",
    "with open(data_file) as f:\n",
    "    X = f.read().split(\"\\n\")\n",
    "    X = np.array(X)\n",
    "    X = X[:-1]   # somehow last entry is empty\n",
    "    X = [parse_expr(xi) for xi in X]\n",
    "\n",
    "with open(taylor_file) as f:\n",
    "    y_taylor = f.read().split(\"\\n\")\n",
    "    y_taylor = np.array(y_taylor)\n",
    "    y_taylor = y_taylor[:-1]\n",
    "    y_taylor = [parse_expr(yi) for yi in y_taylor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(coeffs_file) as f:\n",
    "    y_coeffs = f.read().split(\"\\n\")\n",
    "    y_coeffs = y_coeffs[:-1]\n",
    "    for i, y in enumerate(y_coeffs):\n",
    "        y = parse_expr(y)\n",
    "        # add start end tokens and remove whitespaces\n",
    "        y_coeffs[i] = y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "tmp = [[X[i], y_coeffs[i], y_taylor[i]] for i in range(0, len(X))]\n",
    "random.shuffle(tmp)\n",
    "\n",
    "num_train_samples = int(0.90 * len(tmp))\n",
    "\n",
    "train = tmp[0:num_train_samples]\n",
    "test = tmp[num_train_samples:]\n",
    "X_train = [x[0] for x in train]\n",
    "X_test = [x[0] for x in test]\n",
    "y_taylor_train = [x[2] for x in train]\n",
    "y_taylor_test = [x[2] for x in test]\n",
    "print(len(tmp) == len(train) + len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sympy_tokenize(expr, tokens_list=[], depth=0, parent_ind=None):\n",
    "    if (expr.func == sympy.core.symbol.Symbol) | (expr.func == sympy.core.numbers.Integer):\n",
    "        to_append = expr\n",
    "    else:\n",
    "        to_append = expr.func\n",
    "    tokens_list.append(to_append)\n",
    "    for ind, arg in enumerate(expr.args):\n",
    "        sympy_tokenize(arg, tokens_list, depth+1, parent_ind=ind)\n",
    "    return tokens_list\n",
    "\n",
    "def sympy_tokenize_str(sentence):\n",
    "    Xi_tokenized = sympy_tokenize(sentence, tokens_list=[])\n",
    "    Xi_tokenized_str = [str(el) for el in Xi_tokenized]\n",
    "    return Xi_tokenized_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tokenized_str_train = [sympy_tokenize_str(Xi) for Xi in X_train]\n",
    "y_taylor_tokenized_str_train = [start+sympy_tokenize_str(yi)+end for yi in y_taylor_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_X = Word2Vec(sentences=X_tokenized_str_train, vector_size=100, window=5, min_count=1, workers=4)\n",
    "word2vec_y = Word2Vec(sentences=y_taylor_tokenized_str_train, vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sentence(Xi, model):\n",
    "    # 0 reserved for [end], so add 1 to index\n",
    "    Xi_vectorized = [model.wv.key_to_index[word]+1 for word in Xi]\n",
    "    return Xi_vectorized\n",
    "\n",
    "def vectorize(X_tokenized_str, model, sequence_length=25):\n",
    "    X_vectorized = [ vectorize_sentence(sentence, model) for sentence in X_tokenized_str]\n",
    "    # sequence_length = np.max([len(Xi) for Xi in X_vectorized]) + 5\n",
    "    X_vectorized = [pad_right(Xi, sequence_length, const=0) for Xi in X_vectorized]\n",
    "    return X_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length_X = 50\n",
    "sequence_length_y = 51\n",
    "X_vectorized_train = vectorize(X_tokenized_str_train, word2vec_X, sequence_length=sequence_length_X)\n",
    "y_taylor_vectorized_train = vectorize(y_taylor_tokenized_str_train, word2vec_y, sequence_length=sequence_length_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unvectorize_sentence(Xi, model):\n",
    "    end_ind = np.min(np.where(np.array(Xi) == 0)[0])\n",
    "    Xi_trunc = Xi[0:end_ind]\n",
    "    return [ model.wv.index_to_key[word-1] for word in Xi_trunc]\n",
    "\n",
    "def unvectorize(X_vectorized, model):\n",
    "    X_unvectorized = [unvectorize_sentence(sentence, model) for sentence in X_vectorized]\n",
    "    return X_unvectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "zero-size array to reduction operation minimum which has no identity",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/mknipfer/Documents/UA/TaylorExpansionML/notebooks/2022-05-16-FirstModel.ipynb Cell 15'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mknipfer/Documents/UA/TaylorExpansionML/notebooks/2022-05-16-FirstModel.ipynb#ch0000016vscode-remote?line=0'>1</a>\u001b[0m X_unvectorized_train \u001b[39m=\u001b[39m unvectorize(X_vectorized_train, word2vec_X)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mknipfer/Documents/UA/TaylorExpansionML/notebooks/2022-05-16-FirstModel.ipynb#ch0000016vscode-remote?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(X_unvectorized_train \u001b[39m==\u001b[39m X_tokenized_str_train)\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mknipfer/Documents/UA/TaylorExpansionML/notebooks/2022-05-16-FirstModel.ipynb#ch0000016vscode-remote?line=3'>4</a>\u001b[0m y_taylor_unvectorized_train \u001b[39m=\u001b[39m unvectorize(y_taylor_vectorized_train, word2vec_y)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mknipfer/Documents/UA/TaylorExpansionML/notebooks/2022-05-16-FirstModel.ipynb#ch0000016vscode-remote?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(y_taylor_unvectorized_train \u001b[39m==\u001b[39m y_taylor_tokenized_str_train)\n",
      "\u001b[1;32m/home/mknipfer/Documents/UA/TaylorExpansionML/notebooks/2022-05-16-FirstModel.ipynb Cell 14'\u001b[0m in \u001b[0;36munvectorize\u001b[0;34m(X_vectorized, model)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mknipfer/Documents/UA/TaylorExpansionML/notebooks/2022-05-16-FirstModel.ipynb#ch0000046vscode-remote?line=5'>6</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39munvectorize\u001b[39m(X_vectorized, model):\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mknipfer/Documents/UA/TaylorExpansionML/notebooks/2022-05-16-FirstModel.ipynb#ch0000046vscode-remote?line=6'>7</a>\u001b[0m     X_unvectorized \u001b[39m=\u001b[39m [unvectorize_sentence(sentence, model) \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m X_vectorized]\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mknipfer/Documents/UA/TaylorExpansionML/notebooks/2022-05-16-FirstModel.ipynb#ch0000046vscode-remote?line=7'>8</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m X_unvectorized\n",
      "\u001b[1;32m/home/mknipfer/Documents/UA/TaylorExpansionML/notebooks/2022-05-16-FirstModel.ipynb Cell 14'\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mknipfer/Documents/UA/TaylorExpansionML/notebooks/2022-05-16-FirstModel.ipynb#ch0000046vscode-remote?line=5'>6</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39munvectorize\u001b[39m(X_vectorized, model):\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mknipfer/Documents/UA/TaylorExpansionML/notebooks/2022-05-16-FirstModel.ipynb#ch0000046vscode-remote?line=6'>7</a>\u001b[0m     X_unvectorized \u001b[39m=\u001b[39m [unvectorize_sentence(sentence, model) \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m X_vectorized]\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mknipfer/Documents/UA/TaylorExpansionML/notebooks/2022-05-16-FirstModel.ipynb#ch0000046vscode-remote?line=7'>8</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m X_unvectorized\n",
      "\u001b[1;32m/home/mknipfer/Documents/UA/TaylorExpansionML/notebooks/2022-05-16-FirstModel.ipynb Cell 14'\u001b[0m in \u001b[0;36munvectorize_sentence\u001b[0;34m(Xi, model)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mknipfer/Documents/UA/TaylorExpansionML/notebooks/2022-05-16-FirstModel.ipynb#ch0000046vscode-remote?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39munvectorize_sentence\u001b[39m(Xi, model):\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mknipfer/Documents/UA/TaylorExpansionML/notebooks/2022-05-16-FirstModel.ipynb#ch0000046vscode-remote?line=1'>2</a>\u001b[0m     end_ind \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mmin(np\u001b[39m.\u001b[39;49mwhere(np\u001b[39m.\u001b[39;49marray(Xi) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m)[\u001b[39m0\u001b[39;49m])\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mknipfer/Documents/UA/TaylorExpansionML/notebooks/2022-05-16-FirstModel.ipynb#ch0000046vscode-remote?line=2'>3</a>\u001b[0m     Xi_trunc \u001b[39m=\u001b[39m Xi[\u001b[39m0\u001b[39m:end_ind]\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mknipfer/Documents/UA/TaylorExpansionML/notebooks/2022-05-16-FirstModel.ipynb#ch0000046vscode-remote?line=3'>4</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m [ model\u001b[39m.\u001b[39mwv\u001b[39m.\u001b[39mindex_to_key[word\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m Xi_trunc]\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mamin\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/RandomFunctionGenerator/lib/python3.10/site-packages/numpy/core/fromnumeric.py:2916\u001b[0m, in \u001b[0;36mamin\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   <a href='file:///home/mknipfer/anaconda3/envs/RandomFunctionGenerator/lib/python3.10/site-packages/numpy/core/fromnumeric.py?line=2799'>2800</a>\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_amin_dispatcher)\n\u001b[1;32m   <a href='file:///home/mknipfer/anaconda3/envs/RandomFunctionGenerator/lib/python3.10/site-packages/numpy/core/fromnumeric.py?line=2800'>2801</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mamin\u001b[39m(a, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, keepdims\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39m_NoValue, initial\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39m_NoValue,\n\u001b[1;32m   <a href='file:///home/mknipfer/anaconda3/envs/RandomFunctionGenerator/lib/python3.10/site-packages/numpy/core/fromnumeric.py?line=2801'>2802</a>\u001b[0m          where\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39m_NoValue):\n\u001b[1;32m   <a href='file:///home/mknipfer/anaconda3/envs/RandomFunctionGenerator/lib/python3.10/site-packages/numpy/core/fromnumeric.py?line=2802'>2803</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/mknipfer/anaconda3/envs/RandomFunctionGenerator/lib/python3.10/site-packages/numpy/core/fromnumeric.py?line=2803'>2804</a>\u001b[0m \u001b[39m    Return the minimum of an array or minimum along an axis.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/mknipfer/anaconda3/envs/RandomFunctionGenerator/lib/python3.10/site-packages/numpy/core/fromnumeric.py?line=2804'>2805</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///home/mknipfer/anaconda3/envs/RandomFunctionGenerator/lib/python3.10/site-packages/numpy/core/fromnumeric.py?line=2913'>2914</a>\u001b[0m \u001b[39m    6\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/mknipfer/anaconda3/envs/RandomFunctionGenerator/lib/python3.10/site-packages/numpy/core/fromnumeric.py?line=2914'>2915</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> <a href='file:///home/mknipfer/anaconda3/envs/RandomFunctionGenerator/lib/python3.10/site-packages/numpy/core/fromnumeric.py?line=2915'>2916</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapreduction(a, np\u001b[39m.\u001b[39;49mminimum, \u001b[39m'\u001b[39;49m\u001b[39mmin\u001b[39;49m\u001b[39m'\u001b[39;49m, axis, \u001b[39mNone\u001b[39;49;00m, out,\n\u001b[1;32m   <a href='file:///home/mknipfer/anaconda3/envs/RandomFunctionGenerator/lib/python3.10/site-packages/numpy/core/fromnumeric.py?line=2916'>2917</a>\u001b[0m                           keepdims\u001b[39m=\u001b[39;49mkeepdims, initial\u001b[39m=\u001b[39;49minitial, where\u001b[39m=\u001b[39;49mwhere)\n",
      "File \u001b[0;32m~/anaconda3/envs/RandomFunctionGenerator/lib/python3.10/site-packages/numpy/core/fromnumeric.py:86\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/mknipfer/anaconda3/envs/RandomFunctionGenerator/lib/python3.10/site-packages/numpy/core/fromnumeric.py?line=82'>83</a>\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='file:///home/mknipfer/anaconda3/envs/RandomFunctionGenerator/lib/python3.10/site-packages/numpy/core/fromnumeric.py?line=83'>84</a>\u001b[0m             \u001b[39mreturn\u001b[39;00m reduction(axis\u001b[39m=\u001b[39maxis, out\u001b[39m=\u001b[39mout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpasskwargs)\n\u001b[0;32m---> <a href='file:///home/mknipfer/anaconda3/envs/RandomFunctionGenerator/lib/python3.10/site-packages/numpy/core/fromnumeric.py?line=85'>86</a>\u001b[0m \u001b[39mreturn\u001b[39;00m ufunc\u001b[39m.\u001b[39;49mreduce(obj, axis, dtype, out, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpasskwargs)\n",
      "\u001b[0;31mValueError\u001b[0m: zero-size array to reduction operation minimum which has no identity"
     ]
    }
   ],
   "source": [
    "X_unvectorized_train = unvectorize(X_vectorized_train, word2vec_X)\n",
    "print(X_unvectorized_train == X_tokenized_str_train)\n",
    "\n",
    "y_taylor_unvectorized_train = unvectorize(y_taylor_vectorized_train, word2vec_y)\n",
    "print(y_taylor_unvectorized_train == y_taylor_tokenized_str_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-18 10:03:19.388908: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-05-18 10:03:19.398614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-05-18 10:03:19.399457: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-05-18 10:03:19.400217: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-18 10:03:19.402285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-05-18 10:03:19.403297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-05-18 10:03:19.404801: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-05-18 10:03:19.899614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-05-18 10:03:19.899883: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-05-18 10:03:19.899894: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1609] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2022-05-18 10:03:19.900113: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-05-18 10:03:19.900155: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5430 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070 Ti Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "batch_size=16\n",
    "\n",
    "def format_dataset(X, y):\n",
    "    X = vectorize(X, word2vec_X, sequence_length=sequence_length_X)\n",
    "    y = vectorize(y, word2vec_y, sequence_length=sequence_length_y)\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    return ({\"encoder_inputs\": X, \"decoder_inputs\": y[:, :-1],}, y[:, 1:])\n",
    "\n",
    "def make_dataset(X, y):\n",
    "    dataset = format_dataset(X,y)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(dataset)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()\n",
    "\n",
    "\n",
    "train_ds = make_dataset(X_tokenized_str_train, y_taylor_tokenized_str_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs[\"encoder_inputs\"].shape: (16, 50)\n",
      "inputs[\"decoder_inputs\"].shape: (16, 50)\n",
      "targets.shape: (16, 50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-18 10:03:21.427886: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds.take(1):\n",
    "    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n",
    "    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n",
    "    print(f\"targets.shape: {targets.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super(TransformerEncoder, self).__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=\"int32\")\n",
    "        attention_output = self.attention(\n",
    "            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n",
    "        )\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "\n",
    "\n",
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
    "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=vocab_size, output_dim=embed_dim\n",
    "        )\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=embed_dim\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "\n",
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
    "        super(TransformerDecoder, self).__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.attention_2 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(latent_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.layernorm_3 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
    "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
    "\n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n",
    "        )\n",
    "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "\n",
    "        attention_output_2 = self.attention_2(\n",
    "            query=out_1,\n",
    "            value=encoder_outputs,\n",
    "            key=encoder_outputs,\n",
    "            attention_mask=padding_mask,\n",
    "        )\n",
    "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
    "\n",
    "        proj_output = self.dense_proj(out_2)\n",
    "        return self.layernorm_3(out_2 + proj_output)\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
    "            axis=0,\n",
    "        )\n",
    "        return tf.tile(mask, mult)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_vocab_size = len(word2vec_X.wv.key_to_index)\n",
    "# y_vocab_size = len(word2vec_y.wv.key_to_index)\n",
    "X_vocab_size = 200\n",
    "y_vocab_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 100\n",
    "latent_dim = 200\n",
    "num_heads = 4\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
    "x = PositionalEmbedding(sequence_length_X, X_vocab_size, embed_dim)(encoder_inputs)\n",
    "encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n",
    "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
    "\n",
    "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
    "encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n",
    "x = PositionalEmbedding(sequence_length_X-1, y_vocab_size, embed_dim)(decoder_inputs)\n",
    "x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "decoder_outputs = layers.Dense(X_vocab_size, activation=\"softmax\")(x)\n",
    "decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
    "\n",
    "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
    "transformer = keras.Model(\n",
    "    [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " positional_embedding_4 (Positi  (None, None, 100)   25000       ['encoder_inputs[0][0]']         \n",
      " onalEmbedding)                                                                                   \n",
      "                                                                                                  \n",
      " decoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " transformer_encoder_2 (Transfo  (None, None, 100)   202000      ['positional_embedding_4[0][0]'] \n",
      " rmerEncoder)                                                                                     \n",
      "                                                                                                  \n",
      " model_5 (Functional)           (None, None, 200)    408600      ['decoder_inputs[0][0]',         \n",
      "                                                                  'transformer_encoder_2[0][0]']  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 635,600\n",
      "Trainable params: 635,600\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda_malloc_async\n"
     ]
    }
   ],
   "source": [
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n",
    "print(os.getenv('TF_GPU_ALLOCATOR'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "57/57 [==============================] - 3s 23ms/step - loss: 0.7634 - accuracy: 0.5699\n",
      "Epoch 2/50\n",
      "57/57 [==============================] - 1s 19ms/step - loss: 0.7180 - accuracy: 0.5951\n",
      "Epoch 3/50\n",
      "57/57 [==============================] - 1s 19ms/step - loss: 0.6984 - accuracy: 0.6046\n",
      "Epoch 4/50\n",
      "57/57 [==============================] - 1s 19ms/step - loss: 0.6781 - accuracy: 0.6131\n",
      "Epoch 5/50\n",
      "57/57 [==============================] - 1s 22ms/step - loss: 0.6614 - accuracy: 0.6230\n",
      "Epoch 6/50\n",
      "57/57 [==============================] - 1s 19ms/step - loss: 0.6497 - accuracy: 0.6281\n",
      "Epoch 7/50\n",
      "57/57 [==============================] - 1s 20ms/step - loss: 0.6325 - accuracy: 0.6374\n",
      "Epoch 8/50\n",
      "57/57 [==============================] - 1s 20ms/step - loss: 0.6215 - accuracy: 0.6445\n",
      "Epoch 9/50\n",
      "57/57 [==============================] - 1s 20ms/step - loss: 0.6088 - accuracy: 0.6488\n",
      "Epoch 10/50\n",
      "57/57 [==============================] - 1s 19ms/step - loss: 0.5893 - accuracy: 0.6593\n",
      "Epoch 11/50\n",
      "57/57 [==============================] - 1s 19ms/step - loss: 0.5816 - accuracy: 0.6641\n",
      "Epoch 12/50\n",
      "57/57 [==============================] - 1s 22ms/step - loss: 0.5629 - accuracy: 0.6757\n",
      "Epoch 13/50\n",
      "57/57 [==============================] - 1s 19ms/step - loss: 0.5573 - accuracy: 0.6754\n",
      "Epoch 14/50\n",
      "57/57 [==============================] - 1s 19ms/step - loss: 0.5400 - accuracy: 0.6868\n",
      "Epoch 15/50\n",
      "57/57 [==============================] - 1s 19ms/step - loss: 0.5269 - accuracy: 0.6964\n",
      "Epoch 16/50\n",
      "57/57 [==============================] - 1s 21ms/step - loss: 0.5194 - accuracy: 0.6978\n",
      "Epoch 17/50\n",
      "57/57 [==============================] - 1s 19ms/step - loss: 0.5054 - accuracy: 0.7052\n",
      "Epoch 18/50\n",
      "57/57 [==============================] - 1s 19ms/step - loss: 0.5028 - accuracy: 0.7050\n",
      "Epoch 19/50\n",
      "57/57 [==============================] - 1s 21ms/step - loss: 0.4784 - accuracy: 0.7201\n",
      "Epoch 20/50\n",
      "57/57 [==============================] - 1s 19ms/step - loss: 0.4723 - accuracy: 0.7247\n",
      "Epoch 21/50\n",
      "57/57 [==============================] - 1s 19ms/step - loss: 0.4588 - accuracy: 0.7301\n",
      "Epoch 22/50\n",
      "57/57 [==============================] - 1s 19ms/step - loss: 0.4557 - accuracy: 0.7361\n",
      "Epoch 23/50\n",
      "57/57 [==============================] - 1s 21ms/step - loss: 0.4401 - accuracy: 0.7408\n",
      "Epoch 24/50\n",
      "57/57 [==============================] - 1s 19ms/step - loss: 0.4282 - accuracy: 0.7468\n",
      "Epoch 25/50\n",
      "57/57 [==============================] - 1s 20ms/step - loss: 0.4147 - accuracy: 0.7569\n",
      "Epoch 26/50\n",
      "57/57 [==============================] - 1s 21ms/step - loss: 0.4083 - accuracy: 0.7605\n",
      "Epoch 27/50\n",
      "57/57 [==============================] - 1s 21ms/step - loss: 0.4012 - accuracy: 0.7643\n",
      "Epoch 28/50\n",
      "57/57 [==============================] - 1s 21ms/step - loss: 0.3882 - accuracy: 0.7706\n",
      "Epoch 29/50\n",
      "57/57 [==============================] - 1s 20ms/step - loss: 0.3808 - accuracy: 0.7757\n",
      "Epoch 30/50\n",
      "57/57 [==============================] - 1s 21ms/step - loss: 0.3700 - accuracy: 0.7791\n",
      "Epoch 31/50\n",
      "57/57 [==============================] - 1s 19ms/step - loss: 0.3573 - accuracy: 0.7859\n",
      "Epoch 32/50\n",
      "57/57 [==============================] - 1s 20ms/step - loss: 0.3543 - accuracy: 0.7897\n",
      "Epoch 33/50\n",
      "57/57 [==============================] - 1s 20ms/step - loss: 0.3452 - accuracy: 0.7954\n",
      "Epoch 34/50\n",
      "57/57 [==============================] - 1s 22ms/step - loss: 0.3346 - accuracy: 0.7984\n",
      "Epoch 35/50\n",
      "57/57 [==============================] - 1s 20ms/step - loss: 0.3332 - accuracy: 0.8005\n",
      "Epoch 36/50\n",
      "57/57 [==============================] - 1s 20ms/step - loss: 0.3168 - accuracy: 0.8102\n",
      "Epoch 37/50\n",
      "57/57 [==============================] - 1s 22ms/step - loss: 0.3106 - accuracy: 0.8138\n",
      "Epoch 38/50\n",
      "57/57 [==============================] - 1s 20ms/step - loss: 0.3096 - accuracy: 0.8136\n",
      "Epoch 39/50\n",
      "57/57 [==============================] - 1s 19ms/step - loss: 0.2914 - accuracy: 0.8233\n",
      "Epoch 40/50\n",
      "57/57 [==============================] - 1s 20ms/step - loss: 0.2934 - accuracy: 0.8232\n",
      "Epoch 41/50\n",
      "57/57 [==============================] - 1s 22ms/step - loss: 0.2871 - accuracy: 0.8284\n",
      "Epoch 42/50\n",
      "57/57 [==============================] - 1s 20ms/step - loss: 0.2810 - accuracy: 0.8313\n",
      "Epoch 43/50\n",
      "57/57 [==============================] - 1s 21ms/step - loss: 0.2807 - accuracy: 0.8304\n",
      "Epoch 44/50\n",
      "57/57 [==============================] - 1s 22ms/step - loss: 0.2665 - accuracy: 0.8390\n",
      "Epoch 45/50\n",
      "57/57 [==============================] - 1s 21ms/step - loss: 0.2564 - accuracy: 0.8463\n",
      "Epoch 46/50\n",
      "57/57 [==============================] - 1s 20ms/step - loss: 0.2565 - accuracy: 0.8455\n",
      "Epoch 47/50\n",
      "57/57 [==============================] - 1s 20ms/step - loss: 0.2429 - accuracy: 0.8522\n",
      "Epoch 48/50\n",
      "57/57 [==============================] - 1s 23ms/step - loss: 0.2448 - accuracy: 0.8524\n",
      "Epoch 49/50\n",
      "57/57 [==============================] - 1s 21ms/step - loss: 0.2373 - accuracy: 0.8581\n",
      "Epoch 50/50\n",
      "57/57 [==============================] - 1s 20ms/step - loss: 0.2340 - accuracy: 0.8578\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efe850411e0>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.compile(\n",
    "    \"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "transformer.fit(train_ds, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icecream import ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_decoded_sentence_length = 200\n",
    "def apply_transformer(input_vectorized):\n",
    "    decoded_sentence = [\"[start]\"]\n",
    "    y_index_lookup = word2vec_y.wv.index_to_key\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence = vectorize([decoded_sentence], word2vec_y, sequence_length=sequence_length_X)\n",
    "        tokenized_target_sentence = tf.convert_to_tensor(tokenized_target_sentence)\n",
    "        y_pred = transformer([test_function, tokenized_target_sentence])[0,i,:]\n",
    "        y_index = np.argmax(y_pred) - 1\n",
    "        y = y_index_lookup[y_index]\n",
    "        decoded_sentence = decoded_sentence + [y]\n",
    "\n",
    "        if y == \"[end]\":\n",
    "            break\n",
    "    return  decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[start]',\n",
       " \"<class 'sympy.core.add.Add'>\",\n",
       " \"<class 'sympy.core.mul.Mul'>\",\n",
       " '2',\n",
       " \"<class 'sympy.core.numbers.Exp1'>\",\n",
       " \"<class 'sympy.core.power.Pow'>\",\n",
       " 'x',\n",
       " '2',\n",
       " \"<class 'sympy.core.add.Add'>\",\n",
       " \"<class 'sympy.core.numbers.One'>\",\n",
       " \"<class 'sympy.core.power.Pow'>\",\n",
       " 'tan',\n",
       " \"<class 'sympy.core.numbers.Exp1'>\",\n",
       " '2',\n",
       " \"<class 'sympy.core.mul.Mul'>\",\n",
       " '2',\n",
       " \"<class 'sympy.core.numbers.Exp1'>\",\n",
       " \"<class 'sympy.core.power.Pow'>\",\n",
       " 'cos',\n",
       " \"<class 'sympy.core.numbers.Exp1'>\",\n",
       " '2',\n",
       " \"<class 'sympy.core.numbers.Exp1'>\",\n",
       " \"<class 'sympy.core.mul.Mul'>\",\n",
       " '2',\n",
       " \"<class 'sympy.core.numbers.Exp1'>\",\n",
       " \"<class 'sympy.core.power.Pow'>\",\n",
       " 'x',\n",
       " '4',\n",
       " \"<class 'sympy.core.add.Add'>\",\n",
       " \"<class 'sympy.core.mul.Mul'>\",\n",
       " '8',\n",
       " 'exp',\n",
       " '2',\n",
       " 'tan',\n",
       " \"<class 'sympy.core.numbers.Exp1'>\",\n",
       " '[end]']"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_function = tf.convert_to_tensor([X_vectorized_train[0]])\n",
    "apply_transformer(test_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[start]',\n",
       " \"<class 'sympy.core.add.Add'>\",\n",
       " \"<class 'sympy.core.mul.Mul'>\",\n",
       " '2',\n",
       " \"<class 'sympy.core.numbers.Exp1'>\",\n",
       " \"<class 'sympy.core.power.Pow'>\",\n",
       " 'x',\n",
       " '2',\n",
       " \"<class 'sympy.core.add.Add'>\",\n",
       " \"<class 'sympy.core.numbers.One'>\",\n",
       " \"<class 'sympy.core.power.Pow'>\",\n",
       " 'tan',\n",
       " \"<class 'sympy.core.numbers.Exp1'>\",\n",
       " '2',\n",
       " \"<class 'sympy.core.mul.Mul'>\",\n",
       " \"<class 'sympy.core.numbers.Rational'>\",\n",
       " \"<class 'sympy.core.numbers.Exp1'>\",\n",
       " \"<class 'sympy.core.power.Pow'>\",\n",
       " 'x',\n",
       " '4',\n",
       " \"<class 'sympy.core.add.Add'>\",\n",
       " \"<class 'sympy.core.numbers.One'>\",\n",
       " \"<class 'sympy.core.power.Pow'>\",\n",
       " 'tan',\n",
       " \"<class 'sympy.core.numbers.Exp1'>\",\n",
       " '2',\n",
       " \"<class 'sympy.core.add.Add'>\",\n",
       " '4',\n",
       " \"<class 'sympy.core.mul.Mul'>\",\n",
       " '6',\n",
       " \"<class 'sympy.core.numbers.Exp1'>\",\n",
       " 'tan',\n",
       " \"<class 'sympy.core.numbers.Exp1'>\",\n",
       " 'tan',\n",
       " \"<class 'sympy.core.numbers.Exp1'>\",\n",
       " '[end]']"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_taylor_tokenized_str_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "68a305c3bb66dc100fb8abae87d111d7cb4359f8d5378146abd832e466566bba"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('RandomFunctionGenerator')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
